{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neeraj/miniconda3/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from gensim.test.utils import common_texts\n",
    "from numpy import asarray\n",
    "from numpy import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets for train and test \n",
    "train_data = pd.read_csv('../data/fnc-1/preprocess_train.csv')\n",
    "test_data = pd.read_csv('../data/fnc-1/preprocess_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bodyId</th>\n",
       "      <th>articleHeading</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>articleStance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>soldier shot parliament lock gunfir erupt war ...</td>\n",
       "      <td>small meteorit crash wood area nicaragua capit...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tourist dub spider man spider burrow skin day</td>\n",
       "      <td>small meteorit crash wood area nicaragua capit...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>luke somer kill fail rescu attempt yemen</td>\n",
       "      <td>small meteorit crash wood area nicaragua capit...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>break soldier shot war memori ottawa</td>\n",
       "      <td>small meteorit crash wood area nicaragua capit...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>giant 8ft 9in catfish weigh 19 stone caught it...</td>\n",
       "      <td>small meteorit crash wood area nicaragua capit...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49967</th>\n",
       "      <td>2532</td>\n",
       "      <td>pizza deliveri man get tip 2000 singl deliveri</td>\n",
       "      <td>ann arbor mich pizza deliveri man michigan got...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49968</th>\n",
       "      <td>2532</td>\n",
       "      <td>pizza deliveri man get 2000 tip</td>\n",
       "      <td>ann arbor mich pizza deliveri man michigan got...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49969</th>\n",
       "      <td>2532</td>\n",
       "      <td>luckiest pizza deliveri guy ever get 2000 tip</td>\n",
       "      <td>ann arbor mich pizza deliveri man michigan got...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49970</th>\n",
       "      <td>2532</td>\n",
       "      <td>ann arbor pizza deliveri driver surpris 2000 tip</td>\n",
       "      <td>ann arbor mich pizza deliveri man michigan got...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49971</th>\n",
       "      <td>2532</td>\n",
       "      <td>ann arbor pizza deliveri driver surpris 2000 tip</td>\n",
       "      <td>ann arbor mich pizza deliveri man michigan got...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49972 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bodyId                                     articleHeading  \\\n",
       "0           0  soldier shot parliament lock gunfir erupt war ...   \n",
       "1           0      tourist dub spider man spider burrow skin day   \n",
       "2           0           luke somer kill fail rescu attempt yemen   \n",
       "3           0               break soldier shot war memori ottawa   \n",
       "4           0  giant 8ft 9in catfish weigh 19 stone caught it...   \n",
       "...       ...                                                ...   \n",
       "49967    2532     pizza deliveri man get tip 2000 singl deliveri   \n",
       "49968    2532                    pizza deliveri man get 2000 tip   \n",
       "49969    2532      luckiest pizza deliveri guy ever get 2000 tip   \n",
       "49970    2532   ann arbor pizza deliveri driver surpris 2000 tip   \n",
       "49971    2532   ann arbor pizza deliveri driver surpris 2000 tip   \n",
       "\n",
       "                                             articleBody articleStance  \n",
       "0      small meteorit crash wood area nicaragua capit...     unrelated  \n",
       "1      small meteorit crash wood area nicaragua capit...     unrelated  \n",
       "2      small meteorit crash wood area nicaragua capit...     unrelated  \n",
       "3      small meteorit crash wood area nicaragua capit...     unrelated  \n",
       "4      small meteorit crash wood area nicaragua capit...     unrelated  \n",
       "...                                                  ...           ...  \n",
       "49967  ann arbor mich pizza deliveri man michigan got...         agree  \n",
       "49968  ann arbor mich pizza deliveri man michigan got...         agree  \n",
       "49969  ann arbor mich pizza deliveri man michigan got...         agree  \n",
       "49970  ann arbor mich pizza deliveri man michigan got...         agree  \n",
       "49971  ann arbor mich pizza deliveri man michigan got...         agree  \n",
       "\n",
       "[49972 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Labeled Sentences \n",
    "def get_labeled_sentences(data, label_type):\n",
    "    labeled_data = list()\n",
    "    for i, val in enumerate(data):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled_data.append(TaggedDocument(val.split(), [label]))\n",
    "    return labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articleHeading = get_labeled_sentences(train_data['articleHeading'], 'trainArticleHeading')\n",
    "train_articleBody = get_labeled_sentences(train_data['articleBody'], 'trainArticleBody')\n",
    "test_articleHeading = get_labeled_sentences(test_data['articleHeading'], 'testArticleHeading')\n",
    "test_articleBody = get_labeled_sentences(test_data['articleBody'], 'testArticleBody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['soldier', 'shot', 'parliament', 'lock', 'gunfir', 'erupt', 'war', 'memori'], tags=['trainArticleHeading_0'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_articleHeading[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['small', 'meteorit', 'crash', 'wood', 'area', 'nicaragua', 'capit', 'managua', 'overnight', 'govern', 'said', 'sunday', 'resid', 'report', 'hear', 'mysteri', 'boom', 'left', '16foot', 'deep', 'crater', 'near', 'citi', 'airport', 'associ', 'press', 'report', 'govern', 'spokeswoman', 'rosario', 'murillo', 'said', 'committe', 'form', 'govern', 'studi', 'event', 'determin', 'rel', 'small', 'meteorit', 'appear', 'come', 'asteroid', 'pas', 'close', 'earth', 'house', 'asteroid', '2014', 'rc', 'measur', '60', 'foot', 'diamet', 'skim', 'earth', 'weekend', 'abc', 'news', 'report', 'murillo', 'said', 'nicaragua', 'ask', 'intern', 'expert', 'help', 'local', 'scientist', 'understand', 'happen', 'crater', 'left', 'meteorit', 'radiu', '39', 'foot', 'depth', '16', 'foot', 'said', 'humberto', 'saballo', 'volcanologist', 'nicaraguan', 'institut', 'territori', 'studi', 'committe', 'said', 'still', 'clear', 'meteorit', 'disintegr', 'buri', 'humberto', 'garcia', 'astronomi', 'center', 'nation', 'autonom', 'univers', 'nicaragua', 'said', 'meteorit', 'could', 'relat', 'asteroid', 'forecast', 'pas', 'planet', 'saturday', 'night', 'studi', 'could', 'ice', 'rock', 'said', 'wilfri', 'strauch', 'advis', 'institut', 'territori', 'studi', 'said', 'strang', 'one', 'report', 'streak', 'light', 'ask', 'anyon', 'photo', 'someth', 'local', 'resid', 'report', 'hear', 'loud', 'boom', 'saturday', 'night', 'said', 'didnt', 'see', 'anyth', 'strang', 'sky', 'sit', 'porch', 'saw', 'noth', 'sudden', 'heard', 'larg', 'blast', 'thought', 'bomb', 'felt', 'expans', 'wave', 'jorg', 'santamaria', 'told', 'associ', 'press', 'site', 'crater', 'near', 'managua', 'intern', 'airport', 'air', 'forc', 'base', 'journalist', 'state', 'medium', 'allow', 'visit'], tags=['trainArticleBody_0'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_articleBody[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['appl', 'instal', 'safe', 'instor', 'protect', 'gold', 'watch', 'edit'], tags=['testArticleHeading_0'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_articleHeading[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['alsisi', 'deni', 'isra', 'report', 'state', 'offer', 'extend', 'gaza', 'strip'], tags=['testArticleBody_0'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_articleBody[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150770/150770 [00:00<00:00, 1267602.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating the doc2vec model \n",
    "vector_dimension = 300 \n",
    "# window: The maximum distance between the current and predicted word within a sentence.\n",
    "# alpha: initial learning rate\n",
    "text_model_dbow = Doc2Vec(min_count=1, window=5, vector_size=vector_dimension, sample=1e-4, negative=5, epochs=10, seed=1, alpha=0.065, min_alpha=0.065)\n",
    "# Creating the vocabulary on the train data \n",
    "vocabulary_train_data = train_articleHeading + train_articleBody + test_articleHeading + test_articleBody\n",
    "# Build the vocabulary, number of unique words in heading and body columns of the training dataset\n",
    "text_model_dbow.build_vocab([x for x in tqdm(vocabulary_train_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150770/150770 [00:00<00:00, 2253059.47it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2333608.43it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 1826433.53it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2751946.17it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2717415.58it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2343302.07it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2670999.74it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2804697.80it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2838716.58it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2827761.87it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2714615.95it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2843989.18it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2767288.42it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2795720.55it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2769263.71it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2723606.87it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2807113.11it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2813094.54it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2663911.16it/s]\n",
      "100%|██████████| 150770/150770 [00:00<00:00, 2734713.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train the doc2vec model for 20 epochs \n",
    "for epoch in range(20):\n",
    "    # Using the utils method to shuffle the entire dataset \n",
    "    text_model_dbow.train(utils.shuffle([i for i in tqdm(vocabulary_train_data)]), total_examples=len(vocabulary_train_data), epochs=1)\n",
    "    # Initialise the learning rate to decrease by 0.002 on each epoch \n",
    "    text_model_dbow.alpha -= 0.002\n",
    "    text_model_dbow.min_alpha = text_model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained doc2vec model to get the vectors for the train and test data \n",
    "train_size = len(train_articleHeading)\n",
    "test_size = len(test_articleHeading)\n",
    "# Initialise the training and test vectors as 0 \n",
    "train_articleHeading_arrays = np.zeros((train_size, vector_dimension))\n",
    "train_articleBody_arrays = np.zeros((train_size, vector_dimension))\n",
    "test_articleHeading_arrays = np.zeros((test_size, vector_dimension))\n",
    "test_articleBody_arrays = np.zeros((test_size, vector_dimension))\n",
    "# Method to generate vectors for train and test data \n",
    "def generate_doc2vec_vectors(vectors, text_model, data_size, vector_type):\n",
    "    for i in range(data_size):\n",
    "        title = vector_type + '_' + str(i)\n",
    "        vectors[i] = text_model.docvecs[title]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for train article headings \n",
    "train_articleHeading_vectors = generate_doc2vec_vectors(train_articleHeading_arrays, text_model_dbow, train_size, 'trainArticleHeading')\n",
    "# Get the vectors for train article body\n",
    "train_articleBody_vectors = generate_doc2vec_vectors(train_articleBody_arrays, text_model_dbow, train_size, 'trainArticleBody')\n",
    "# Get the vectors for test article headings \n",
    "test_articleHeading_vectors = generate_doc2vec_vectors(test_articleHeading_arrays, text_model_dbow, test_size, 'testArticleHeading')\n",
    "# Get the vectors for test article body\n",
    "test_articleBody_vectors = generate_doc2vec_vectors(test_articleBody_arrays, text_model_dbow, test_size, 'testArticleBody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create doc2vec vector for all the documents in a defined order and append them together.\n",
    "train_featured_vectors = np.squeeze(np.c_[train_articleHeading_vectors, train_articleBody_vectors])\n",
    "test_featured_vectors = np.squeeze(np.c_[test_articleHeading_vectors, test_articleBody_vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the final featured vectors into numpy array\n",
    "train_featured_vectors = np.array(train_featured_vectors)\n",
    "test_featured_vectors = np.array(test_featured_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the X_train and Y_train for doc2vec models \n",
    "save('../data/fnc-1/x_train_doc2vec.npy', train_featured_vectors)\n",
    "# Save the X_test numpy array \n",
    "save('../data/fnc-1/x_test_doc2vec.npy', test_featured_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
