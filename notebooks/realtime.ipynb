{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/neeraj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the X_train, Y_train, X_test and Y_test\n",
    "train_data = pd.read_csv('../data/fnc-1/final_train.csv')\n",
    "test_data = pd.read_csv('../data/fnc-1/final_test.csv')\n",
    "X_train = np.load('../data/fnc-1/x_train.npy')\n",
    "Y_train = np.load('../data/fnc-1/y_train.npy', allow_pickle=True)\n",
    "# Converting the labels into int type to prevent unknown type error \n",
    "Y_train = Y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data, remove punctuations and all \n",
    "# Clean the datasets \n",
    "def clean_data(text):\n",
    "    # Remove whitespaces\n",
    "    text = text.strip()\n",
    "    # Remove special characters and numbers\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    # Remove punctuation \n",
    "    text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove square brackets \n",
    "    text = re.sub('\\[[^]]*\\][.;:!\\'?,\\\"()\\[\\]] ', '', text)\n",
    "    # Remove break elements from the text \n",
    "    text = re.sub(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\", '', text)\n",
    "    # Converting the text to lowercase \n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Remove stopwords \n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def remove_stopwords(text):\n",
    "    final_words = [word for word in text if word not in stop_words]\n",
    "    return \" \".join(final_words)\n",
    "\n",
    "# Stemming to reduce words to their word stem for train data using \n",
    "# Porter Stemming or Lancaster Stemming algorithms.\n",
    "def perform_stemming(text):\n",
    "    # Using PorterStemmer \n",
    "    porter = PorterStemmer()\n",
    "    porterFinal = [porter.stem(word) for word in text.split()]\n",
    "    text = \" \".join(porterFinal)\n",
    "    # Using LancasterStemmer \n",
    "    # lancaster = LancasterStemmer()\n",
    "    # lancasterFinal = [lancaster.stem(word) for word in text.split()]\n",
    "    # text = \" \".join(lancasterFinal)\n",
    "    return text\n",
    "\n",
    "# Lemmatization to reduce inflectional forms to a common base form\n",
    "def perform_lemmatization(text):\n",
    "    # Using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizerFinal = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    text = \" \".join(lemmatizerFinal)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create term document matrix fo the columns, tf-idf\n",
    "def create_term_document_matrix(df_type, tf, tfidf):\n",
    "    final_array = list()\n",
    "    for i, val in df_type.iterrows():\n",
    "        normalised_articleHeading = val['articleHeading']\n",
    "        normalised_articleBody = val['articleBody']\n",
    "        # Transform article heading to document-term matrix for tf\n",
    "        term_document_matrix_heading_tf = tf.transform([normalised_articleHeading])\n",
    "        # Return a ndarray such that the new shape should be compatible with the original shape\n",
    "        term_document_matrix_heading_tf = term_document_matrix_heading_tf.toarray().reshape(1, -1)\n",
    "        # Transform article body to document-term matrix for tf\n",
    "        term_document_matrix_body_tf = tf.transform([normalised_articleBody])\n",
    "        # Return a ndarray such that the new shape should be compatible with the original shape\n",
    "        term_document_matrix_body_tf = term_document_matrix_body_tf.toarray().reshape(1, -1)\n",
    "        # Transform article heading to document-term matrix for tf-idf \n",
    "        term_document_matrix_heading_tfidf = tfidf.transform([normalised_articleHeading])\n",
    "        # Return the ndarray for the tf-idf of article headings\n",
    "        term_document_matrix_heading_tfidf =  term_document_matrix_heading_tfidf.toarray()\n",
    "        # Transform article body to document-term matrix for tf-idf \n",
    "        term_document_matrix_body_tfidf = tfidf.transform([normalised_articleBody])\n",
    "        # Return the ndarray for the tf-idf of article body\n",
    "        term_document_matrix_body_tfidf =  term_document_matrix_body_tfidf.toarray()\n",
    "        # Get the cosine similarity \n",
    "        term_document_matrix_cosine_similarity = cosine_similarity(term_document_matrix_heading_tfidf, term_document_matrix_body_tfidf)\n",
    "        # Transform into the original shape \n",
    "        term_document_matrix_cosine_similarity = term_document_matrix_cosine_similarity.reshape(1, -1)\n",
    "        # Get the final featured vectors \n",
    "        featured_vectors = np.squeeze(np.c_[term_document_matrix_heading_tf, term_document_matrix_body_tf, term_document_matrix_cosine_similarity])\n",
    "#         featured_vectors = np.squeeze(np.c_[term_document_matrix_heading_tfidf, term_document_matrix_body_tfidf, term_document_matrix_cosine_similarity])\n",
    "        # Append the featured vectors to the final data array \n",
    "        final_array.append(featured_vectors)\n",
    "    # Convert the final array into numpy array \n",
    "    final_array = np.array(final_array)\n",
    "    return final_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the combined unqiue strings in headings and body\n",
    "def fetch_final_strings_combined(df_type):\n",
    "    final_strings_combined = list()\n",
    "    # Loop over each column and append the values \n",
    "    for i, val in enumerate(df_type['articleHeading']):\n",
    "        if val not in final_strings_combined:\n",
    "            final_strings_combined.append(val)\n",
    "    for i, val in enumerate(df_type['articleBody']):\n",
    "        if val not in final_strings_combined:\n",
    "            final_strings_combined.append(val)\n",
    "    # Return the final combined array of unique strings\n",
    "    return final_strings_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the common vocabulary of strings for train data\n",
    "train_vocabulary = fetch_final_strings_combined(train_data)\n",
    "# Learn vocabulary training set.\n",
    "tf = TfidfVectorizer(max_features = 2500, use_idf = False)\n",
    "count_train_tfvectorizer = tf.fit(train_vocabulary)\n",
    "# Learn vocabulary and idf from training set.\n",
    "tfidf = TfidfVectorizer(max_features = 2500, use_idf = True)\n",
    "count_train_tfidfvectorizer = tfidf.fit(train_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = clean_data(text)\n",
    "    text = word_tokenize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = perform_stemming(text)\n",
    "    text = perform_lemmatization(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logitsic Regression Classifier \n",
    "logisticRegression = LogisticRegression(solver='saga', multi_class='multinomial')\n",
    "logisticRegression.fit(X_train, Y_train)\n",
    "\n",
    "def main():\n",
    "    print('Enter the article heading: ')\n",
    "    head = input('\\n')\n",
    "    print('Enter the article body')\n",
    "    body = input('\\n')\n",
    "    head = preprocess(head)\n",
    "    body = preprocess(body)\n",
    "    data = [[head, body]] \n",
    "    df = pd.DataFrame(data, columns = ['articleHeading', 'articleBody']) \n",
    "    X_test = create_term_document_matrix(df, tf, tfidf)\n",
    "    lr_Y_predictions = logisticRegression.predict(X_test)\n",
    "    return lr_Y_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the article heading: \n",
      "\n",
      "Does the Obama Foundation Own 82% of Mail-In Ballot Printers?\n",
      "Enter the article body\n",
      "\n",
      "Hailing it as a major step in changing humanity’s prevailing forms of meat consumption, Singapore on Friday announced governmental approval of the first-ever lab-grilled chicken. “After years of research and testing, consumers will now be able purchase delicious lab-grilled meat,” said Singapore Food Agency spokesperson Kuan Sim, adding that the approval was the first step to replacing chicken grilled in restaurants and the home with lab-grilled chicken that was just as succulent and tasty as th\n",
      "\n",
      "\n",
      "\n",
      "Unrelated\n"
     ]
    }
   ],
   "source": [
    "final = main()\n",
    "print('\\n\\n')\n",
    "if final[0] == 0:\n",
    "    print('Agree')\n",
    "elif final[0] == 1:\n",
    "    print('Disagree')\n",
    "elif final[0] == 2:\n",
    "    print('Discuss')\n",
    "else:\n",
    "    print('Unrelated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No gender segregation on Saudia'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['articleHeading'][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Saudi Arabian Airlines on Sunday dismissed claims made by some local media outlets that the national flag carrier is planning to segregate men and women on its flights.\\nDenying that such arrangements are being considered, Abdullah Al-Ajhar, the airline’s spokesman, termed the reports as “false” and “misleading.”\\nSpeaking to Arab News, Al-Ajhar asserted that there are no plans to separate passengers based on their gender.\\nA few days ago, international media, quoting a local daily, reported that Saudia is planning to segregate according to gender, following complaints of uncomfortable journeys by male relatives of female passengers.\\nAccording to the report, some passengers complained that females sitting next to non-related male passengers felt uncomfortable.\\nIn the report, the international news organization quoted Abdul Rahman Al-Fahd, airline’s vice president for marketing, as saying that measures would be taken to solve this problem.\\nHowever, the official concerned denied having made any statement on segregation. He asserted that his quote was taken out of context.\\nAl-Fahd tried to clear the air on the matter by tweeting what he claimed was his real response: “I answered: We are trying to find a solution and awaiting the outcome.”\\nIn a significant but unrelated move, Saudia has extended the validity of domestic flight tickets from 6 months to 1 year from Jan. 1.\\nإعلان\\nWhat's this? inRead™ invented by Teads.tv\\n\\nAccording to the new system, passengers will have to purchase tickets while making reservations on domestic flights, with the ticket validity extended to one year to give passengers enough flexibility, a Saudia official said.\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['articleBody'][404]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agree'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['articleStance'][404]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
